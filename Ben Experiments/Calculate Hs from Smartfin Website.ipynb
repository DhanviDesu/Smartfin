{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purisa Jasmine Simmons\n",
    "#Viren Abhyankar\n",
    "#February 2020\n",
    "\n",
    "#Overview: Trying to generate a PSD plot (framework for future plots).\n",
    "#Based on Method V of this paper: https://journals.ametsoc.org/doi/pdf/10.1175/2010JTECHO724.1\n",
    "\n",
    "#First, parse the data from the .CSV file.\n",
    "#This data comes from Scripps buoy that recalibrates every 30 minutes \n",
    "#all of the vertical accelerations are contained in IMUA2.\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = 'C:/Users/USERNAME/Anaconda3/Lib/site-packages/mpl_toolkits/basemap'\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from scipy import constants\n",
    "from scipy import signal #added\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import simps\n",
    "from scipy.integrate import cumtrapz\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "import peakutils\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "from plotly import tools #added all the plotly's\n",
    "import plotly.offline\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import math #added\n",
    "import re   #added\n",
    "import statistics #added\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# For the definition of the abstract base class IMU_Base\n",
    "import abc\n",
    "\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rid:  15692\n",
      "ocean_csv_url:  https://surf.smartfin.org/media/201811/google_105349665704999793400_0006667E229D_181109191556_Ocean.CSV\n",
      "motion_csv_url:  https://surf.smartfin.org/media/201811/google_105349665704999793400_0006667E229D_181109191556_Motion.CSV\n",
      "\n",
      "\n",
      "\n",
      "motion_df_small raw:                                     UTC        Time  IMU A1  IMU A2  IMU A3  \\\n",
      "0     2018-11-09 19:16:03.806000+00:00  1414742884     NaN     NaN     NaN   \n",
      "1     2018-11-09 19:16:03.809000+00:00  1414742887   493.0    48.0   110.0   \n",
      "2     2018-11-09 19:16:04.061000+00:00  1414743138   513.0    89.0    62.0   \n",
      "3     2018-11-09 19:16:04.312000+00:00  1414743387   494.0    92.0    80.0   \n",
      "4     2018-11-09 19:16:04.565000+00:00  1414743639   421.0   205.0  -104.0   \n",
      "...                                ...         ...     ...     ...     ...   \n",
      "22552 2018-11-09 20:38:14.334000+00:00  1419643689   501.0   -11.0    99.0   \n",
      "22553 2018-11-09 20:38:14.500000+00:00  1419643854     NaN     NaN     NaN   \n",
      "22554 2018-11-09 20:38:14.587000+00:00  1419643940   502.0   -11.0    99.0   \n",
      "22555 2018-11-09 20:38:14.839000+00:00  1419644191   501.0   -13.0    99.0   \n",
      "22556 2018-11-09 20:38:15.093000+00:00  1419644443   501.0   -11.0    98.0   \n",
      "\n",
      "       IMU G1  IMU G2  IMU G3  IMU M1  IMU M2  IMU M3   Latitude   Longitude  \n",
      "0         NaN     NaN     NaN     NaN     NaN     NaN  3285871.0 -11725690.0  \n",
      "1        75.0  -124.0   -86.0  -309.0   209.0    39.0        NaN         NaN  \n",
      "2        34.0   -36.0   -92.0  -320.0   194.0    38.0        NaN         NaN  \n",
      "3        69.0   -63.0   -42.0  -329.0   189.0    49.0        NaN         NaN  \n",
      "4       192.0   -92.0   -37.0  -330.0   180.0    64.0        NaN         NaN  \n",
      "...       ...     ...     ...     ...     ...     ...        ...         ...  \n",
      "22552     9.0    20.0     2.0  -303.0   267.0    37.0        NaN         NaN  \n",
      "22553     NaN     NaN     NaN     NaN     NaN     NaN  3287082.0 -11722116.0  \n",
      "22554    10.0    21.0     1.0  -308.0   262.0    32.0        NaN         NaN  \n",
      "22555    10.0    21.0     1.0  -310.0   260.0    38.0        NaN         NaN  \n",
      "22556    10.0    20.0     2.0  -309.0   265.0    35.0        NaN         NaN  \n",
      "\n",
      "[22557 rows x 13 columns]\n",
      "\n",
      "\n",
      "\n",
      "ocean_df_small raw:                                   UTC        Time  Temperature 1  \\\n",
      "0   2018-11-09 19:16:03.362000+00:00  1414742443            375   \n",
      "1   2018-11-09 19:16:09.419000+00:00  1414748463            377   \n",
      "2   2018-11-09 19:16:15.475000+00:00  1414754483            375   \n",
      "3   2018-11-09 19:16:21.533000+00:00  1414760504            374   \n",
      "4   2018-11-09 19:16:27.591000+00:00  1414766526            369   \n",
      "..                               ...         ...            ...   \n",
      "810 2018-11-09 20:37:49.208000+00:00  1419618714            450   \n",
      "811 2018-11-09 20:37:55.263000+00:00  1419624733            449   \n",
      "812 2018-11-09 20:38:01.320000+00:00  1419630753            448   \n",
      "813 2018-11-09 20:38:07.376000+00:00  1419636773            447   \n",
      "814 2018-11-09 20:38:13.433000+00:00  1419642793            446   \n",
      "\n",
      "     Calibrated Temperature 1  Temperature 1 Stable  Temperature 2  \\\n",
      "0                      23.438                 False           5207   \n",
      "1                      23.562                 False           5239   \n",
      "2                      23.438                 False           5250   \n",
      "3                      23.375                 False           5250   \n",
      "4                      23.062                 False           5111   \n",
      "..                        ...                   ...            ...   \n",
      "810                    28.125                 False           6662   \n",
      "811                    28.062                 False           6635   \n",
      "812                    28.000                 False           6608   \n",
      "813                    27.938                 False           6587   \n",
      "814                    27.875                 False           6560   \n",
      "\n",
      "     Calibrated Temperature 2  Temperature 2 Stable  salinity  \\\n",
      "0                      20.512                 False       NaN   \n",
      "1                      20.637                 False       NaN   \n",
      "2                      20.680                 False       NaN   \n",
      "3                      20.680                 False       NaN   \n",
      "4                      20.135                 False       NaN   \n",
      "..                        ...                   ...       ...   \n",
      "810                    26.217                 False       NaN   \n",
      "811                    26.111                 False       NaN   \n",
      "812                    26.005                 False       NaN   \n",
      "813                    25.923                 False       NaN   \n",
      "814                    25.817                 False       NaN   \n",
      "\n",
      "     Calibrated Salinity  Salinity Stable  pH  Calibrated pH  pH Stable  \\\n",
      "0                    NaN              NaN NaN            NaN        NaN   \n",
      "1                    NaN              NaN NaN            NaN        NaN   \n",
      "2                    NaN              NaN NaN            NaN        NaN   \n",
      "3                    NaN              NaN NaN            NaN        NaN   \n",
      "4                    NaN              NaN NaN            NaN        NaN   \n",
      "..                   ...              ...  ..            ...        ...   \n",
      "810                  NaN              NaN NaN            NaN        NaN   \n",
      "811                  NaN              NaN NaN            NaN        NaN   \n",
      "812                  NaN              NaN NaN            NaN        NaN   \n",
      "813                  NaN              NaN NaN            NaN        NaN   \n",
      "814                  NaN              NaN NaN            NaN        NaN   \n",
      "\n",
      "      elapsed  \n",
      "0       0.000  \n",
      "1       6.057  \n",
      "2      12.113  \n",
      "3      18.171  \n",
      "4      24.229  \n",
      "..        ...  \n",
      "810  4905.846  \n",
      "811  4911.901  \n",
      "812  4917.958  \n",
      "813  4924.014  \n",
      "814  4930.071  \n",
      "\n",
      "[815 rows x 15 columns]\n",
      "\n",
      "\n",
      "\n",
      "ocean_df_small length pre upsample:  815\n",
      "motion_df_small length pre upsample:  22557\n",
      "ocean_df_resample length:  149397\n",
      "motion_df_resample length:  149434\n",
      "\n",
      "\n",
      "\n",
      "motion df length pre na drop:  149434\n",
      "motion_df_dropped length post na drop:  21645\n",
      "\n",
      "\n",
      "\n",
      "motion_df_dropped:                                                    Time  IMU A1  IMU A2  \\\n",
      "ride_id UTC                                                              \n",
      "15692   2018-11-09 19:16:03.789000+00:00  1.414743e+09   493.0    48.0   \n",
      "        2018-11-09 19:16:04.053000+00:00  1.414743e+09   513.0    89.0   \n",
      "        2018-11-09 19:16:04.284000+00:00  1.414743e+09   494.0    92.0   \n",
      "        2018-11-09 19:16:04.548000+00:00  1.414744e+09   421.0   205.0   \n",
      "        2018-11-09 19:16:04.812000+00:00  1.414744e+09   534.0   306.0   \n",
      "...                                                ...     ...     ...   \n",
      "        2018-11-09 20:38:14.055000+00:00  1.419643e+09   501.0   -11.0   \n",
      "        2018-11-09 20:38:14.319000+00:00  1.419644e+09   501.0   -11.0   \n",
      "        2018-11-09 20:38:14.583000+00:00  1.419644e+09   502.0   -11.0   \n",
      "        2018-11-09 20:38:14.814000+00:00  1.419644e+09   501.0   -13.0   \n",
      "        2018-11-09 20:38:15.078000+00:00  1.419644e+09   501.0   -11.0   \n",
      "\n",
      "                                          IMU A3  IMU G1  IMU G2  IMU G3  \\\n",
      "ride_id UTC                                                                \n",
      "15692   2018-11-09 19:16:03.789000+00:00   110.0    75.0  -124.0   -86.0   \n",
      "        2018-11-09 19:16:04.053000+00:00    62.0    34.0   -36.0   -92.0   \n",
      "        2018-11-09 19:16:04.284000+00:00    80.0    69.0   -63.0   -42.0   \n",
      "        2018-11-09 19:16:04.548000+00:00  -104.0   192.0   -92.0   -37.0   \n",
      "        2018-11-09 19:16:04.812000+00:00   -32.0  -421.0  -233.0  -229.0   \n",
      "...                                          ...     ...     ...     ...   \n",
      "        2018-11-09 20:38:14.055000+00:00    99.0     9.0    21.0     2.0   \n",
      "        2018-11-09 20:38:14.319000+00:00    99.0     9.0    20.0     2.0   \n",
      "        2018-11-09 20:38:14.583000+00:00    99.0    10.0    21.0     1.0   \n",
      "        2018-11-09 20:38:14.814000+00:00    99.0    10.0    21.0     1.0   \n",
      "        2018-11-09 20:38:15.078000+00:00    98.0    10.0    20.0     2.0   \n",
      "\n",
      "                                          IMU M1  IMU M2  IMU M3  \n",
      "ride_id UTC                                                       \n",
      "15692   2018-11-09 19:16:03.789000+00:00  -309.0   209.0    39.0  \n",
      "        2018-11-09 19:16:04.053000+00:00  -320.0   194.0    38.0  \n",
      "        2018-11-09 19:16:04.284000+00:00  -329.0   189.0    49.0  \n",
      "        2018-11-09 19:16:04.548000+00:00  -330.0   180.0    64.0  \n",
      "        2018-11-09 19:16:04.812000+00:00  -325.0   161.0    97.0  \n",
      "...                                          ...     ...     ...  \n",
      "        2018-11-09 20:38:14.055000+00:00  -293.0   259.0    41.0  \n",
      "        2018-11-09 20:38:14.319000+00:00  -303.0   267.0    37.0  \n",
      "        2018-11-09 20:38:14.583000+00:00  -308.0   262.0    32.0  \n",
      "        2018-11-09 20:38:14.814000+00:00  -310.0   260.0    38.0  \n",
      "        2018-11-09 20:38:15.078000+00:00  -309.0   265.0    35.0  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21645 rows x 10 columns]\n",
      "ocean_df:                                                    Time  Temperature 1  \\\n",
      "ride_id UTC                                                             \n",
      "15692   2018-11-09 19:16:03.360000+00:00  1.414742e+09          375.0   \n",
      "        2018-11-09 19:16:03.393000+00:00           NaN            NaN   \n",
      "        2018-11-09 19:16:03.426000+00:00           NaN            NaN   \n",
      "        2018-11-09 19:16:03.459000+00:00           NaN            NaN   \n",
      "        2018-11-09 19:16:03.492000+00:00           NaN            NaN   \n",
      "...                                                ...            ...   \n",
      "        2018-11-09 20:38:13.296000+00:00           NaN            NaN   \n",
      "        2018-11-09 20:38:13.329000+00:00           NaN            NaN   \n",
      "        2018-11-09 20:38:13.362000+00:00           NaN            NaN   \n",
      "        2018-11-09 20:38:13.395000+00:00           NaN            NaN   \n",
      "        2018-11-09 20:38:13.428000+00:00  1.419643e+09          446.0   \n",
      "\n",
      "                                          Calibrated Temperature 1  \\\n",
      "ride_id UTC                                                          \n",
      "15692   2018-11-09 19:16:03.360000+00:00                    23.438   \n",
      "        2018-11-09 19:16:03.393000+00:00                       NaN   \n",
      "        2018-11-09 19:16:03.426000+00:00                       NaN   \n",
      "        2018-11-09 19:16:03.459000+00:00                       NaN   \n",
      "        2018-11-09 19:16:03.492000+00:00                       NaN   \n",
      "...                                                            ...   \n",
      "        2018-11-09 20:38:13.296000+00:00                       NaN   \n",
      "        2018-11-09 20:38:13.329000+00:00                       NaN   \n",
      "        2018-11-09 20:38:13.362000+00:00                       NaN   \n",
      "        2018-11-09 20:38:13.395000+00:00                       NaN   \n",
      "        2018-11-09 20:38:13.428000+00:00                    27.875   \n",
      "\n",
      "                                          Temperature 1 Stable  Temperature 2  \\\n",
      "ride_id UTC                                                                     \n",
      "15692   2018-11-09 19:16:03.360000+00:00                   0.0         5207.0   \n",
      "        2018-11-09 19:16:03.393000+00:00                   NaN            NaN   \n",
      "        2018-11-09 19:16:03.426000+00:00                   NaN            NaN   \n",
      "        2018-11-09 19:16:03.459000+00:00                   NaN            NaN   \n",
      "        2018-11-09 19:16:03.492000+00:00                   NaN            NaN   \n",
      "...                                                        ...            ...   \n",
      "        2018-11-09 20:38:13.296000+00:00                   NaN            NaN   \n",
      "        2018-11-09 20:38:13.329000+00:00                   NaN            NaN   \n",
      "        2018-11-09 20:38:13.362000+00:00                   NaN            NaN   \n",
      "        2018-11-09 20:38:13.395000+00:00                   NaN            NaN   \n",
      "        2018-11-09 20:38:13.428000+00:00                   0.0         6560.0   \n",
      "\n",
      "                                          Calibrated Temperature 2  \\\n",
      "ride_id UTC                                                          \n",
      "15692   2018-11-09 19:16:03.360000+00:00                    20.512   \n",
      "        2018-11-09 19:16:03.393000+00:00                       NaN   \n",
      "        2018-11-09 19:16:03.426000+00:00                       NaN   \n",
      "        2018-11-09 19:16:03.459000+00:00                       NaN   \n",
      "        2018-11-09 19:16:03.492000+00:00                       NaN   \n",
      "...                                                            ...   \n",
      "        2018-11-09 20:38:13.296000+00:00                       NaN   \n",
      "        2018-11-09 20:38:13.329000+00:00                       NaN   \n",
      "        2018-11-09 20:38:13.362000+00:00                       NaN   \n",
      "        2018-11-09 20:38:13.395000+00:00                       NaN   \n",
      "        2018-11-09 20:38:13.428000+00:00                    25.817   \n",
      "\n",
      "                                          Temperature 2 Stable  salinity  \\\n",
      "ride_id UTC                                                                \n",
      "15692   2018-11-09 19:16:03.360000+00:00                   0.0       NaN   \n",
      "        2018-11-09 19:16:03.393000+00:00                   NaN       NaN   \n",
      "        2018-11-09 19:16:03.426000+00:00                   NaN       NaN   \n",
      "        2018-11-09 19:16:03.459000+00:00                   NaN       NaN   \n",
      "        2018-11-09 19:16:03.492000+00:00                   NaN       NaN   \n",
      "...                                                        ...       ...   \n",
      "        2018-11-09 20:38:13.296000+00:00                   NaN       NaN   \n",
      "        2018-11-09 20:38:13.329000+00:00                   NaN       NaN   \n",
      "        2018-11-09 20:38:13.362000+00:00                   NaN       NaN   \n",
      "        2018-11-09 20:38:13.395000+00:00                   NaN       NaN   \n",
      "        2018-11-09 20:38:13.428000+00:00                   0.0       NaN   \n",
      "\n",
      "                                          Calibrated Salinity  \\\n",
      "ride_id UTC                                                     \n",
      "15692   2018-11-09 19:16:03.360000+00:00                  NaN   \n",
      "        2018-11-09 19:16:03.393000+00:00                  NaN   \n",
      "        2018-11-09 19:16:03.426000+00:00                  NaN   \n",
      "        2018-11-09 19:16:03.459000+00:00                  NaN   \n",
      "        2018-11-09 19:16:03.492000+00:00                  NaN   \n",
      "...                                                       ...   \n",
      "        2018-11-09 20:38:13.296000+00:00                  NaN   \n",
      "        2018-11-09 20:38:13.329000+00:00                  NaN   \n",
      "        2018-11-09 20:38:13.362000+00:00                  NaN   \n",
      "        2018-11-09 20:38:13.395000+00:00                  NaN   \n",
      "        2018-11-09 20:38:13.428000+00:00                  NaN   \n",
      "\n",
      "                                          Salinity Stable  pH  Calibrated pH  \\\n",
      "ride_id UTC                                                                    \n",
      "15692   2018-11-09 19:16:03.360000+00:00              NaN NaN            NaN   \n",
      "        2018-11-09 19:16:03.393000+00:00              NaN NaN            NaN   \n",
      "        2018-11-09 19:16:03.426000+00:00              NaN NaN            NaN   \n",
      "        2018-11-09 19:16:03.459000+00:00              NaN NaN            NaN   \n",
      "        2018-11-09 19:16:03.492000+00:00              NaN NaN            NaN   \n",
      "...                                                   ...  ..            ...   \n",
      "        2018-11-09 20:38:13.296000+00:00              NaN NaN            NaN   \n",
      "        2018-11-09 20:38:13.329000+00:00              NaN NaN            NaN   \n",
      "        2018-11-09 20:38:13.362000+00:00              NaN NaN            NaN   \n",
      "        2018-11-09 20:38:13.395000+00:00              NaN NaN            NaN   \n",
      "        2018-11-09 20:38:13.428000+00:00              NaN NaN            NaN   \n",
      "\n",
      "                                          pH Stable   elapsed  \n",
      "ride_id UTC                                                    \n",
      "15692   2018-11-09 19:16:03.360000+00:00        NaN     0.000  \n",
      "        2018-11-09 19:16:03.393000+00:00        NaN       NaN  \n",
      "        2018-11-09 19:16:03.426000+00:00        NaN       NaN  \n",
      "        2018-11-09 19:16:03.459000+00:00        NaN       NaN  \n",
      "        2018-11-09 19:16:03.492000+00:00        NaN       NaN  \n",
      "...                                             ...       ...  \n",
      "        2018-11-09 20:38:13.296000+00:00        NaN       NaN  \n",
      "        2018-11-09 20:38:13.329000+00:00        NaN       NaN  \n",
      "        2018-11-09 20:38:13.362000+00:00        NaN       NaN  \n",
      "        2018-11-09 20:38:13.395000+00:00        NaN       NaN  \n",
      "        2018-11-09 20:38:13.428000+00:00        NaN  4930.071  \n",
      "\n",
      "[149397 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "plt.rc(\"font\", size=14) \n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "ride_ids = ['15692']\n",
    "\n",
    "\n",
    "#%% Fin ID scraper\n",
    "# Input fin ID, get all ride IDs\n",
    "# base URL to which we'll append given fin IDs\n",
    "# fin_url_base = 'http://surf.smartfin.org/fin/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "# str_id_ride = 'rideId = \\'' # backslash allows us to look for single quote\n",
    "# str_id_date = 'var date = \\'' # backslash allows us to look for single quote\n",
    "\n",
    "#%% Ride ID scraper\n",
    "\n",
    "\n",
    "# Input ride ID, get ocean and motion CSVs\n",
    "# Base URL to which we'll append given ride IDs\n",
    "ride_url_base = 'https://surf.smartfin.org/ride/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below to get csv id \n",
    "str_id_csv = 'img id=\"temperatureChart\" class=\"chart\" src=\"' \n",
    "\n",
    "\n",
    "def get_csv_from_ride_id(rid):\n",
    "    \n",
    "# step 1    \n",
    "    # Build URL for each individual ride\n",
    "    ride_url = ride_url_base+str(rid)\n",
    "#     print(\"ride_url: \" + ride_url)\n",
    "    \n",
    "# step 2\n",
    "    # query smartfin website to retrieve the ride's webpage in HTML  \n",
    "    html_contents = requests.get(ride_url).text\n",
    "#     print(\"html contents: \", html_contents)\n",
    "    \n",
    "    # Find CSV file location id in html page by csv file tag\n",
    "    loc_csv_id = html_contents.find(str_id_csv)\n",
    "#     print(\"loc_csv_id: \", loc_csv_id)\n",
    "    \n",
    "# step 3\n",
    "    # log into smartfin website to get request authentication\n",
    "    # Different based on whether user logged in with FB or Google\n",
    "    offset_googleOAuth = [46, 114]\n",
    "    offset_facebkOAuth = [46, 112]\n",
    "    if html_contents[loc_csv_id+59] == 'f': # Facebook login\n",
    "        off0 = offset_facebkOAuth[0]\n",
    "        off1 = offset_facebkOAuth[1]\n",
    "    else: # Google login\n",
    "        off0 = offset_googleOAuth[0]\n",
    "        off1 = offset_googleOAuth[1]\n",
    "\n",
    "# step 4\n",
    "    # use csv id and authentication offsets to build query string\n",
    "    csv_id_longstr = html_contents[loc_csv_id+off0:loc_csv_id+off1]\n",
    "#     print(\"csv_id_longstr: \", csv_id_longstr)\n",
    "    \n",
    "    # Stitch together full URL for CSV\n",
    "    if (\"media\" in csv_id_longstr) & (\"Calibration\" not in html_contents): # other junk URLs can exist and break everything\n",
    "\n",
    "# step 5\n",
    "        # full urls to get csv file       \n",
    "        ocean_csv_url = f'https://surf.smartfin.org/{csv_id_longstr}Ocean.CSV'\n",
    "        motion_csv_url = f'https://surf.smartfin.org/{csv_id_longstr}Motion.CSV'\n",
    "        \n",
    "        print(\"ocean_csv_url: \", ocean_csv_url)\n",
    "        print(\"motion_csv_url: \", motion_csv_url)\n",
    "        print('\\n\\n')\n",
    "\n",
    "# step 6\n",
    "        # Go to ocean_csv_url and grab contents (theoretically, a CSV)\n",
    "        ocean_df_small = pd.read_csv(ocean_csv_url, parse_dates = [0])\n",
    "        motion_df_small = pd.read_csv(motion_csv_url, parse_dates = [0])\n",
    "\n",
    "\n",
    "# step 7\n",
    "        # 7a. add elasped column to show how much time has elapsed since first reading\n",
    "        elapsed_timedelta = (ocean_df_small['UTC']-ocean_df_small['UTC'][0])\n",
    "        ocean_df_small['elapsed'] = elapsed_timedelta/np.timedelta64(1, 's')\n",
    "        \n",
    "        print(\"motion_df_small raw: \", motion_df_small)\n",
    "        print('\\n\\n')\n",
    "        print(\"ocean_df_small raw: \", ocean_df_small)\n",
    "        print('\\n\\n')\n",
    "\n",
    "\n",
    "        \n",
    "        # 7b. make the index of each df the timestamp\n",
    "        if len(ocean_df_small) > 1:\n",
    "            ocean_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            motion_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            \n",
    "            print(\"ocean_df_small length pre upsample: \", len(ocean_df_small))\n",
    "            print(\"motion_df_small length pre upsample: \", len(motion_df_small))\n",
    "            \n",
    "            # 7c. resample data to 33ms intervals (30 Hz)\n",
    "            #May need to change this sampling interval:\n",
    "            sample_interval = '33ms'\n",
    "                        \n",
    "            ocean_df_small_resample = ocean_df_small.resample(sample_interval).mean()\n",
    "            motion_df_small_resample = motion_df_small.resample(sample_interval).mean()\n",
    "    \n",
    "            \n",
    "            print('ocean_df_resample length: ', len(ocean_df_small_resample))\n",
    "            print('motion_df_resample length: ', len(motion_df_small_resample))\n",
    "            print('\\n\\n')\n",
    "            \n",
    "            # returns all rows that have values in latitude column           \n",
    "            # No need to save many extra rows with no fix\n",
    "            # motion_df_small = motion_df_small[~np.isnan(motion_df_small.Latitude)]\n",
    "            \n",
    "            return ocean_df_small_resample, motion_df_small_resample\n",
    "\n",
    "    # if dataframe is empty, just return empty dataframe    \n",
    "    else:\n",
    "        ocean_df_small_resample = pd.DataFrame() # empty DF just so something is returned\n",
    "        motion_df_small_resample = pd.DataFrame() \n",
    "        return ocean_df_small_resample, motion_df_small_resample\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# actual script\n",
    "\n",
    "appended_ocean_list = [] # list of DataFrames from original CSVs\n",
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "## Nested loops (for each fin ID, find all ride IDs, then build a DataFrame from all ride CSVs)\n",
    "## (Here, ride IDS are either ocean or motion dataframes)\n",
    "count_good_fins = 0      # number of dataframes with non empty data\n",
    "    \n",
    "# Loop over ride_ids and find CSVs\n",
    "for rid in ride_ids:\n",
    "    print(\"rid: \", rid)\n",
    "    try:\n",
    "        # runs code from function defined above\n",
    "        new_ocean_df, new_motion_df = get_csv_from_ride_id(rid) # get given ride's CSV from its ride ID using function above\n",
    "        \n",
    "        # for each non empty df, append to list of already created dataframes        \n",
    "        if not new_ocean_df.empty: # Calibration rides, for example\n",
    "            \n",
    "            # Append only if DF isn't empty. There may be a better way to control empty DFs which are created above\n",
    "            appended_multiIndex.append(str(rid)) # build list to be multiIndex of future DataFrame\n",
    "            appended_ocean_list.append(new_ocean_df)\n",
    "            appended_motion_list.append(new_motion_df)\n",
    "            \n",
    "            count_good_fins += 1\n",
    "        \n",
    "    except: \n",
    "        print(\"Ride threw an exception!\")\n",
    "        #print(\"Ride \", rid, \"threw an exception!\")    \n",
    "\n",
    "#%% Build the \"Master\" DataFrame\n",
    "# keys for each diferent dataframe in the big dataframes\n",
    "df_keys = tuple(appended_multiIndex) # keys gotta be a tuple, a list which data in it cannot be changed\n",
    "\n",
    "# concatinate all dataframes in each list into one big dataframe\n",
    "ocean_df = pd.concat(appended_ocean_list, keys = df_keys, names=['ride_id'])\n",
    "motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])\n",
    "\n",
    "\n",
    "##Here, maybe just use info from the motion_df and don't worry about ocean_df data for now.\n",
    "##If you do want ocean_df data, look at how Phil was getting it from \"July 10th and 11th Calibration\" jupyter notebook file.\n",
    "#We can also check to see if the surfboard was recording \"in-water-freq\" or \n",
    "#\"out-of-water-freq\" based on how many NaN values we see. \n",
    "\n",
    "\n",
    "\n",
    "# 7d. clear na values from dataframes\n",
    "#Drop the latitude and longitude values since most of them are Nan:\n",
    "print('motion df length pre na drop: ', len(motion_df))\n",
    "motion_df_dropped = motion_df.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "#Drop the NAN values from the motion data:\n",
    "motion_df_dropped = motion_df_dropped.dropna(axis=0, how='any')\n",
    "print('motion_df_dropped length post na drop: ', len(motion_df_dropped))\n",
    "print('\\n\\n')\n",
    "\n",
    "# finished clean dataframes\n",
    "print('motion_df_dropped: ', motion_df_dropped)\n",
    "print('ocean_df: ', ocean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              UTC        Time  IMU A1  IMU A2  IMU A3  IMU G1  \\\n",
      "0  2018-11-09T19:16:03.8060+00:00  1414742884     NaN     NaN     NaN     NaN   \n",
      "1  2018-11-09T19:16:03.8090+00:00  1414742887   493.0    48.0   110.0    75.0   \n",
      "2  2018-11-09T19:16:04.0610+00:00  1414743138   513.0    89.0    62.0    34.0   \n",
      "3  2018-11-09T19:16:04.3120+00:00  1414743387   494.0    92.0    80.0    69.0   \n",
      "4  2018-11-09T19:16:04.5650+00:00  1414743639   421.0   205.0  -104.0   192.0   \n",
      "\n",
      "   IMU G2  IMU G3  IMU M1  IMU M2  IMU M3   Latitude   Longitude  \n",
      "0     NaN     NaN     NaN     NaN     NaN  3285871.0 -11725690.0  \n",
      "1  -124.0   -86.0  -309.0   209.0    39.0        NaN         NaN  \n",
      "2   -36.0   -92.0  -320.0   194.0    38.0        NaN         NaN  \n",
      "3   -63.0   -42.0  -329.0   189.0    49.0        NaN         NaN  \n",
      "4   -92.0   -37.0  -330.0   180.0    64.0        NaN         NaN  \n",
      "22557\n"
     ]
    }
   ],
   "source": [
    "## Read CSV from URL\n",
    "\n",
    "#Gets all columns\n",
    "#link will come from 'motion_csv_url' from fin ID\n",
    "\n",
    "motion_df = pd.read_csv('https://surf.smartfin.org/media/201811/google_105349665704999793400_0006667E229D_181109191556_Motion.CSV')\n",
    "# motion_df.dropna()\n",
    "# #motion_df.drop(['Latitude'], axis=1)\n",
    "# #motion_df = motion_df.drop(['Time'], axis=1)\n",
    "print(motion_df.head())\n",
    "print(len(motion_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_copy_motion_df = motion_df.copy(deep=True) #make a copy of the dataframe with raw data included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_array = []\n",
    "utc_time = motion_df[['UTC']].to_numpy()\n",
    "\n",
    "\n",
    "START = utc_time[0]\n",
    "END = utc_time[-1]\n",
    "\n",
    "start_index = np.where(utc_time==START)[0][0]\n",
    "end_index = np.where(utc_time==END)[0][0]\n",
    "\n",
    "length = end_index-start_index\n",
    "\n",
    "mins_per_measure = 60/length\n",
    "sec_per_measure = 60*mins_per_measure\n",
    "\n",
    "for i in range(0, length):\n",
    "    time_array.append(sec_per_measure*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse time string and calculate half hour\n",
    "def add_half_hour(time_str):\n",
    "    # Parse string\n",
    "    hrs = int(time_str[:2])\n",
    "    mins = int(time_str[3:5])\n",
    "    \n",
    "    # Set minutes and hours\n",
    "    if (mins < 30):\n",
    "        mins += 30\n",
    "    else:\n",
    "        mins -= 30\n",
    "        if (hrs == 23):\n",
    "            hrs = 0\n",
    "        else:\n",
    "            hrs += 1\n",
    "            \n",
    "    # Check if mins and hrs are single digits\n",
    "    if (mins < 10):\n",
    "        mins = '0'+str(mins)\n",
    "    if (hrs < 10):\n",
    "        hrs = '0'+str(hrs)\n",
    "        \n",
    "    return str(hrs)+':'+str(mins)\n",
    "\n",
    "# Parse int and compare functions in case we need them in the future\n",
    "\n",
    "def parse_int(time_str):\n",
    "    return 60*int(time_str[:2])+int(time_str[3:5])\n",
    "\n",
    "def is_less_than_eq(str1, str2):\n",
    "    if (str1[:2]=='23' and str2[:2]=='00'):\n",
    "        return True\n",
    "    \n",
    "    str1 = parse_int(str1)\n",
    "    str2 = parse_int(str2)\n",
    "    \n",
    "    return (str1 <= str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-017e223be2c4>\u001b[0m in \u001b[0;36madd_half_hour\u001b[1;34m(time_str)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madd_half_hour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Parse string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mhrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Goal: create an array of arrays of IMU A2 data ###\n",
    "surf_sessions = []\n",
    "height_labels = []\n",
    "utc_labels = []\n",
    "date_labels = []\n",
    "\n",
    "## Step 1: get 5 columns of the data frame to iterate over - ID, Time, IMU A2, Date, Height ##\n",
    "df_parser = motion_df[['UTC', 'Time', 'IMU A2']]\n",
    "\n",
    "# Convert to numpy array\n",
    "time_parser = df_parser[['Time']].to_numpy()\n",
    "utc_parser = df_parser[['UTC']].to_numpy()\n",
    "imuA2_parser = df_parser[['IMU A2']].to_numpy()\n",
    "\n",
    "## Step 2: loop through every data point ##\n",
    "working_array = []\n",
    "skip_check = '00:00'\n",
    "end_index_check = None\n",
    "\n",
    "for start_index in range(0, len(time_parser)):\n",
    "    ## Goal 1: Append IMU data if we are in the half-hour segment ##\n",
    "    if (end_index_check != None):\n",
    "        # Check if we've reached the end of the half hour before appending\n",
    "        if (start_index==end_index_check):\n",
    "            surf_sessions.append(working_array)\n",
    "            working_array = []\n",
    "            end_index_check = None\n",
    "        else:\n",
    "            working_array.append(imuA2_parser[start_index][0])\n",
    "        continue\n",
    "        \n",
    "    ## Goal 2: Check if height of current and half-hour times are equal ##\n",
    "    start = time_parser[start_index][0]\n",
    "    \n",
    "    # Skip through a minute if it has already been checked and invalidated\n",
    "    if (start==skip_check):\n",
    "        continue\n",
    "        \n",
    "    # Set end point\n",
    "    end = add_half_hour(start)\n",
    "    \n",
    "    # Check if ride is still valid for half hour time\n",
    "    try:\n",
    "        # End index is where the time is equal to end and we are in the same ride as we started\n",
    "        end_index = np.where((time_parser==end) & (id_parser==id_parser[start_index]))[0][0]\n",
    "    except IndexError:\n",
    "        # Skip through minutes\n",
    "        skip_check = start\n",
    "        continue\n",
    "    else:\n",
    "        # (occurs after try block) skip if the heights don't match\n",
    "        if (height_parser[start_index]!=height_parser[end_index-1]):\n",
    "            skip_check = start\n",
    "        else:\n",
    "            # add to array if heights do match\n",
    "            working_array.append(imuA2_parser[start_index][0])\n",
    "            height_labels.append(height_parser[start_index][0])\n",
    "            utc_labels.append(utc_parser[start_index][0])\n",
    "            date_labels.append(date_parser[start_index][0])\n",
    "            \n",
    "            end_index_check = end_index\n",
    "            skip_check = '00:00'\n",
    "\n",
    "            \n",
    "            print(\"Session: \", len(surf_sessions)+1, \", Date: \", date_parser[start_index][0])\n",
    "            print(\"At \", start, \" and \", end)\n",
    "            print(\"Start: \", height_parser[start_index], \" and End: \", height_parser[end_index-1], \"\\n\")\n",
    "\n",
    "print(\"Total sessions: \", len(surf_sessions))\n",
    "print(\"Total heights: \", len(height_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
