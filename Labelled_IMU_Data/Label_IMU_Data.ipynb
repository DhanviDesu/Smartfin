{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a logistic regression model in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Logistic%20Regression%20balanced.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evans\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc(\"font\", size=14) \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "import peakutils\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import requests\n",
    "\n",
    "#Read data from a local csv file:\n",
    "\n",
    "##Will change this to scrape files from the Smartfin.org website later.\n",
    "#data = pd.read_csv('Motion_13735.CSV', header=0)   \n",
    "#data = data.dropna()\n",
    "\n",
    "#Print out the column headings:\n",
    "#print(data.shape)\n",
    "#print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of specific ride IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure the ride_id and the footage file match up:\n",
    "ride_ids = ['15692']\n",
    "footage_file = './Labelled_Footage/Footage3.txt'\n",
    "\n",
    "\n",
    "#ride_ids = ['14827']\n",
    "# 14743 - Motion Control July 10th\n",
    "# 14750 - Magnetometer Control July 11th\n",
    "# 14814 - Pool Displacement Control July 17th\n",
    "# 14815 - Compass Orientation (Lying on Charger Side) July 19th\n",
    "# 14816 - Orientation w Higher Sampling (Lying on Charger Side) July 20th\n",
    "# 14827 - Pool Displacement Control w Higher Sampling (Jul 23)\n",
    "# 14888 - First Buoy Calibration Experiment (July 30)\n",
    "# 15218 - Jasmine's Second Ride Sesh filmed with GoPro (Aug 29) //no footage\n",
    "# 15629 - Jasmine's First Ride Sesh filmed with VIRB (Oct. 24) //first labelled footage!\n",
    "# 15669 - Jasmine's Second Ride Sesh filmed with VIRB (Nov. 7) //second labelled footage!\n",
    "# 15692 - Jasmine's 3rd Ride Sesh filmed with VIRB (Nov. 9) //third labelled footage!\n",
    "# 15686 - Jasmine's 4th Ride Sesh filmed with VIRB (Nov. 11) //fourth labelled footage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin ID Scraper (pulls dataframes for specific ride id from website):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://surf.smartfin.org/ride/15692\n",
      "https://surf.smartfin.org/media/201811/google_105349665704999793400_0006667E229D_181109191556_Ocean.CSV\n",
      "149397\n",
      "149434\n",
      "Ride data has been uploaded.\n",
      "                                                  Time  IMU A1  IMU A2  \\\n",
      "ride_id UTC                                                              \n",
      "15692   2018-11-09 19:16:03.789000+00:00  1.414743e+09   493.0    48.0   \n",
      "        2018-11-09 19:16:03.822000+00:00           NaN     NaN     NaN   \n",
      "        2018-11-09 19:16:03.855000+00:00           NaN     NaN     NaN   \n",
      "        2018-11-09 19:16:03.888000+00:00           NaN     NaN     NaN   \n",
      "        2018-11-09 19:16:03.921000+00:00           NaN     NaN     NaN   \n",
      "...                                                ...     ...     ...   \n",
      "        2018-11-09 20:38:14.946000+00:00           NaN     NaN     NaN   \n",
      "        2018-11-09 20:38:14.979000+00:00           NaN     NaN     NaN   \n",
      "        2018-11-09 20:38:15.012000+00:00           NaN     NaN     NaN   \n",
      "        2018-11-09 20:38:15.045000+00:00           NaN     NaN     NaN   \n",
      "        2018-11-09 20:38:15.078000+00:00  1.419644e+09   501.0   -11.0   \n",
      "\n",
      "                                          IMU A3  IMU G1  IMU G2  IMU G3  \\\n",
      "ride_id UTC                                                                \n",
      "15692   2018-11-09 19:16:03.789000+00:00   110.0    75.0  -124.0   -86.0   \n",
      "        2018-11-09 19:16:03.822000+00:00     NaN     NaN     NaN     NaN   \n",
      "        2018-11-09 19:16:03.855000+00:00     NaN     NaN     NaN     NaN   \n",
      "        2018-11-09 19:16:03.888000+00:00     NaN     NaN     NaN     NaN   \n",
      "        2018-11-09 19:16:03.921000+00:00     NaN     NaN     NaN     NaN   \n",
      "...                                          ...     ...     ...     ...   \n",
      "        2018-11-09 20:38:14.946000+00:00     NaN     NaN     NaN     NaN   \n",
      "        2018-11-09 20:38:14.979000+00:00     NaN     NaN     NaN     NaN   \n",
      "        2018-11-09 20:38:15.012000+00:00     NaN     NaN     NaN     NaN   \n",
      "        2018-11-09 20:38:15.045000+00:00     NaN     NaN     NaN     NaN   \n",
      "        2018-11-09 20:38:15.078000+00:00    98.0    10.0    20.0     2.0   \n",
      "\n",
      "                                          IMU M1  IMU M2  IMU M3   Latitude  \\\n",
      "ride_id UTC                                                                   \n",
      "15692   2018-11-09 19:16:03.789000+00:00  -309.0   209.0    39.0  3285871.0   \n",
      "        2018-11-09 19:16:03.822000+00:00     NaN     NaN     NaN        NaN   \n",
      "        2018-11-09 19:16:03.855000+00:00     NaN     NaN     NaN        NaN   \n",
      "        2018-11-09 19:16:03.888000+00:00     NaN     NaN     NaN        NaN   \n",
      "        2018-11-09 19:16:03.921000+00:00     NaN     NaN     NaN        NaN   \n",
      "...                                          ...     ...     ...        ...   \n",
      "        2018-11-09 20:38:14.946000+00:00     NaN     NaN     NaN        NaN   \n",
      "        2018-11-09 20:38:14.979000+00:00     NaN     NaN     NaN        NaN   \n",
      "        2018-11-09 20:38:15.012000+00:00     NaN     NaN     NaN        NaN   \n",
      "        2018-11-09 20:38:15.045000+00:00     NaN     NaN     NaN        NaN   \n",
      "        2018-11-09 20:38:15.078000+00:00  -309.0   265.0    35.0        NaN   \n",
      "\n",
      "                                           Longitude  \n",
      "ride_id UTC                                           \n",
      "15692   2018-11-09 19:16:03.789000+00:00 -11725690.0  \n",
      "        2018-11-09 19:16:03.822000+00:00         NaN  \n",
      "        2018-11-09 19:16:03.855000+00:00         NaN  \n",
      "        2018-11-09 19:16:03.888000+00:00         NaN  \n",
      "        2018-11-09 19:16:03.921000+00:00         NaN  \n",
      "...                                              ...  \n",
      "        2018-11-09 20:38:14.946000+00:00         NaN  \n",
      "        2018-11-09 20:38:14.979000+00:00         NaN  \n",
      "        2018-11-09 20:38:15.012000+00:00         NaN  \n",
      "        2018-11-09 20:38:15.045000+00:00         NaN  \n",
      "        2018-11-09 20:38:15.078000+00:00         NaN  \n",
      "\n",
      "[149434 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#%% Fin ID scraper\n",
    "# Input fin ID, get all ride IDs\n",
    "# base URL to which we'll append given fin IDs\n",
    "fin_url_base = 'http://surf.smartfin.org/fin/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "str_id_ride = 'rideId = \\'' # backslash allows us to look for single quote\n",
    "str_id_date = 'var date = \\'' # backslash allows us to look for single quote\n",
    "\n",
    "#%% Ride ID scraper\n",
    "# Input ride ID, get ocean and motion CSVs\n",
    "# Base URL to which we'll append given ride IDs\n",
    "ride_url_base = 'https://surf.smartfin.org/ride/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "str_id_csv = 'img id=\"temperatureChart\" class=\"chart\" src=\"' \n",
    "\n",
    "def get_csv_from_ride_id(rid):\n",
    "    # Build URL for each individual ride\n",
    "    ride_url = ride_url_base+str(rid)\n",
    "    print(ride_url)\n",
    "    \n",
    "    # Get contents of ride_url\n",
    "    html_contents = requests.get(ride_url).text\n",
    "    \n",
    "    # Find CSV identifier \n",
    "    loc_csv_id = html_contents.find(str_id_csv)\n",
    "    \n",
    "    # Different based on whether user logged in with FB or Google\n",
    "    offset_googleOAuth = [46, 114]\n",
    "    offset_facebkOAuth = [46, 112]\n",
    "    if html_contents[loc_csv_id+59] == 'f': # Facebook login\n",
    "        off0 = offset_facebkOAuth[0]\n",
    "        off1 = offset_facebkOAuth[1]\n",
    "    else: # Google login\n",
    "        off0 = offset_googleOAuth[0]\n",
    "        off1 = offset_googleOAuth[1]\n",
    "        \n",
    "    csv_id_longstr = html_contents[loc_csv_id+off0:loc_csv_id+off1]\n",
    "    \n",
    "#    print(csv_id_longstr)\n",
    "    \n",
    "    # Stitch together full URL for CSV\n",
    "    if (\"media\" in csv_id_longstr) & (\"Calibration\" not in html_contents): # other junk URLs can exist and break everything\n",
    "        \n",
    "        ocean_csv_url = 'https://surf.smartfin.org/'+csv_id_longstr+'Ocean.CSV'\n",
    "        motion_csv_url = 'https://surf.smartfin.org/'+csv_id_longstr+'Motion.CSV'\n",
    "        \n",
    "        print(ocean_csv_url)\n",
    "        # Go to ocean_csv_url and grab contents (theoretically, a CSV)\n",
    "        ocean_df_small = pd.read_csv(ocean_csv_url, parse_dates = [0])\n",
    "        elapsed_timedelta = (ocean_df_small['UTC']-ocean_df_small['UTC'][0])\n",
    "        ocean_df_small['elapsed'] = elapsed_timedelta/np.timedelta64(1, 's')\n",
    "        \n",
    "        motion_df_small = pd.read_csv(motion_csv_url, parse_dates = [0])\n",
    "        \n",
    "        # Reindex on timestamp if there are at least a few rows\n",
    "        if len(ocean_df_small) > 1:\n",
    "            ocean_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            motion_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            \n",
    "            #print(ocean_df_small)\n",
    "            #print(motion_df_small)\n",
    "            \n",
    "            #May need to change this sampling interval:\n",
    "            sample_interval = '33ms'\n",
    "            \n",
    "            \n",
    "            ocean_df_small_resample = ocean_df_small.resample(sample_interval).mean()\n",
    "            motion_df_small_resample = motion_df_small.resample(sample_interval).mean()\n",
    "            \n",
    "            # No need to save many extra rows with no fix\n",
    "            motion_df_small = motion_df_small[~np.isnan(motion_df_small.Latitude)]\n",
    "            \n",
    "            return ocean_df_small_resample, motion_df_small_resample\n",
    "\n",
    "    else:\n",
    "        ocean_df_small_resample = pd.DataFrame() # empty DF just so something is returned\n",
    "        motion_df_small_resample = pd.DataFrame() \n",
    "        return ocean_df_small_resample, motion_df_small_resample\n",
    "    \n",
    "appended_ocean_list = [] # list of DataFrames from original CSVs\n",
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "## Nested loops (for each fin ID, find all ride IDs, then build a DataFrame from all ride CSVs)\n",
    "## (Here, ride IDS are either ocean or motion dataframes)\n",
    "count_good_fins = 0\n",
    "    \n",
    "# Loop over ride_ids and find CSVs\n",
    "for rid in ride_ids:\n",
    "    try:\n",
    "        new_ocean_df, new_motion_df = get_csv_from_ride_id(rid) # get given ride's CSV from its ride ID using function above\n",
    "        print(len(new_ocean_df))\n",
    "        print(len(new_motion_df))\n",
    "        if not new_ocean_df.empty: # Calibration rides, for example\n",
    "            # Append only if DF isn't empty. There may be a better way to control empty DFs which are created above\n",
    "            appended_multiIndex.append(str(rid)) # build list to be multiIndex of future DataFrame\n",
    "            appended_ocean_list.append(new_ocean_df)\n",
    "            appended_motion_list.append(new_motion_df)\n",
    "            print(\"Ride data has been uploaded.\")\n",
    "            #print(\"Ride: \", rid, \"data has been uploaded.\")\n",
    "            count_good_fins += 1\n",
    "        \n",
    "    except: \n",
    "        print(\"Ride threw an exception!\")\n",
    "        #print(\"Ride \", rid, \"threw an exception!\")    \n",
    "\n",
    "#%% Build the \"Master\" DataFrame\n",
    "\n",
    "# appended_ocean_df.summary()\n",
    "df_keys = tuple(appended_multiIndex) # keys gotta be a tuple, a list which data in it cannot be changed\n",
    "ocean_df = pd.concat(appended_ocean_list, keys = df_keys, names=['ride_id'])\n",
    "motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])\n",
    "\n",
    "\n",
    "##Here, maybe just use info from the motion_df and don't worry about ocean_df data for now.\n",
    "##If you do want ocean_df data, look at how Phil was getting it from \"July 10th and 11th Calibration\" jupyter notebook file.\n",
    "\n",
    "#We can also check to see if the surfboard was recording \"in-water-freq\" or \n",
    "#\"out-of-water-freq\" based on how many NaN values we see. \n",
    "print(motion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the NA values from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_df_dropped length:  21645\n",
      "                                                  Time  IMU A1  IMU A2  \\\n",
      "ride_id UTC                                                              \n",
      "15692   2018-11-09 19:16:03.789000+00:00  1.414743e+09   493.0    48.0   \n",
      "        2018-11-09 19:16:04.053000+00:00  1.414743e+09   513.0    89.0   \n",
      "        2018-11-09 19:16:04.284000+00:00  1.414743e+09   494.0    92.0   \n",
      "        2018-11-09 19:16:04.548000+00:00  1.414744e+09   421.0   205.0   \n",
      "        2018-11-09 19:16:04.812000+00:00  1.414744e+09   534.0   306.0   \n",
      "...                                                ...     ...     ...   \n",
      "        2018-11-09 20:38:14.055000+00:00  1.419643e+09   501.0   -11.0   \n",
      "        2018-11-09 20:38:14.319000+00:00  1.419644e+09   501.0   -11.0   \n",
      "        2018-11-09 20:38:14.583000+00:00  1.419644e+09   502.0   -11.0   \n",
      "        2018-11-09 20:38:14.814000+00:00  1.419644e+09   501.0   -13.0   \n",
      "        2018-11-09 20:38:15.078000+00:00  1.419644e+09   501.0   -11.0   \n",
      "\n",
      "                                          IMU A3  IMU G1  IMU G2  IMU G3  \\\n",
      "ride_id UTC                                                                \n",
      "15692   2018-11-09 19:16:03.789000+00:00   110.0    75.0  -124.0   -86.0   \n",
      "        2018-11-09 19:16:04.053000+00:00    62.0    34.0   -36.0   -92.0   \n",
      "        2018-11-09 19:16:04.284000+00:00    80.0    69.0   -63.0   -42.0   \n",
      "        2018-11-09 19:16:04.548000+00:00  -104.0   192.0   -92.0   -37.0   \n",
      "        2018-11-09 19:16:04.812000+00:00   -32.0  -421.0  -233.0  -229.0   \n",
      "...                                          ...     ...     ...     ...   \n",
      "        2018-11-09 20:38:14.055000+00:00    99.0     9.0    21.0     2.0   \n",
      "        2018-11-09 20:38:14.319000+00:00    99.0     9.0    20.0     2.0   \n",
      "        2018-11-09 20:38:14.583000+00:00    99.0    10.0    21.0     1.0   \n",
      "        2018-11-09 20:38:14.814000+00:00    99.0    10.0    21.0     1.0   \n",
      "        2018-11-09 20:38:15.078000+00:00    98.0    10.0    20.0     2.0   \n",
      "\n",
      "                                          IMU M1  IMU M2  IMU M3  \n",
      "ride_id UTC                                                       \n",
      "15692   2018-11-09 19:16:03.789000+00:00  -309.0   209.0    39.0  \n",
      "        2018-11-09 19:16:04.053000+00:00  -320.0   194.0    38.0  \n",
      "        2018-11-09 19:16:04.284000+00:00  -329.0   189.0    49.0  \n",
      "        2018-11-09 19:16:04.548000+00:00  -330.0   180.0    64.0  \n",
      "        2018-11-09 19:16:04.812000+00:00  -325.0   161.0    97.0  \n",
      "...                                          ...     ...     ...  \n",
      "        2018-11-09 20:38:14.055000+00:00  -293.0   259.0    41.0  \n",
      "        2018-11-09 20:38:14.319000+00:00  -303.0   267.0    37.0  \n",
      "        2018-11-09 20:38:14.583000+00:00  -308.0   262.0    32.0  \n",
      "        2018-11-09 20:38:14.814000+00:00  -310.0   260.0    38.0  \n",
      "        2018-11-09 20:38:15.078000+00:00  -309.0   265.0    35.0  \n",
      "\n",
      "[21645 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#Drop the latitude and longitude values since most of them are Nan:\n",
    "motion_df_dropped = motion_df.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "\n",
    "#Drop the NAN values from the motion data:\n",
    "motion_df_dropped = motion_df_dropped.dropna(axis=0, how='any')\n",
    "print('motion_df_dropped length: ', len(motion_df_dropped))\n",
    "print(motion_df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an elapsed time field to sync Smartfin data with Video Footage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>IMU A1</th>\n",
       "      <th>IMU A2</th>\n",
       "      <th>IMU A3</th>\n",
       "      <th>IMU G1</th>\n",
       "      <th>IMU G2</th>\n",
       "      <th>IMU G3</th>\n",
       "      <th>IMU M1</th>\n",
       "      <th>IMU M2</th>\n",
       "      <th>IMU M3</th>\n",
       "      <th>TimeDelta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ride_id</th>\n",
       "      <th>UTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">15692</th>\n",
       "      <th>2018-11-09 19:16:03.789000+00:00</th>\n",
       "      <td>1.414743e+09</td>\n",
       "      <td>493.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>-86.0</td>\n",
       "      <td>-309.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:04.053000+00:00</th>\n",
       "      <td>1.414743e+09</td>\n",
       "      <td>513.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>-320.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>252.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:04.284000+00:00</th>\n",
       "      <td>1.414743e+09</td>\n",
       "      <td>494.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-329.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>501.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:04.548000+00:00</th>\n",
       "      <td>1.414744e+09</td>\n",
       "      <td>421.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-104.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>-330.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>753.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:04.812000+00:00</th>\n",
       "      <td>1.414744e+09</td>\n",
       "      <td>534.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-421.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-229.0</td>\n",
       "      <td>-325.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1003.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:05.043000+00:00</th>\n",
       "      <td>1.414744e+09</td>\n",
       "      <td>455.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>-102.0</td>\n",
       "      <td>-355.0</td>\n",
       "      <td>-376.0</td>\n",
       "      <td>-397.0</td>\n",
       "      <td>-337.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1253.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:05.307000+00:00</th>\n",
       "      <td>1.414744e+09</td>\n",
       "      <td>474.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>-219.0</td>\n",
       "      <td>-234.0</td>\n",
       "      <td>-527.0</td>\n",
       "      <td>-465.0</td>\n",
       "      <td>-311.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1504.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:05.571000+00:00</th>\n",
       "      <td>1.414745e+09</td>\n",
       "      <td>363.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>-131.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-662.0</td>\n",
       "      <td>-305.0</td>\n",
       "      <td>-238.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>1755.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:05.802000+00:00</th>\n",
       "      <td>1.414745e+09</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>-447.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>-643.0</td>\n",
       "      <td>-153.0</td>\n",
       "      <td>-159.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>2006.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09 19:16:06.066000+00:00</th>\n",
       "      <td>1.414745e+09</td>\n",
       "      <td>35.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>-132.0</td>\n",
       "      <td>-114.0</td>\n",
       "      <td>-430.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-86.0</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2258.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Time  IMU A1  IMU A2  \\\n",
       "ride_id UTC                                                              \n",
       "15692   2018-11-09 19:16:03.789000+00:00  1.414743e+09   493.0    48.0   \n",
       "        2018-11-09 19:16:04.053000+00:00  1.414743e+09   513.0    89.0   \n",
       "        2018-11-09 19:16:04.284000+00:00  1.414743e+09   494.0    92.0   \n",
       "        2018-11-09 19:16:04.548000+00:00  1.414744e+09   421.0   205.0   \n",
       "        2018-11-09 19:16:04.812000+00:00  1.414744e+09   534.0   306.0   \n",
       "        2018-11-09 19:16:05.043000+00:00  1.414744e+09   455.0   149.0   \n",
       "        2018-11-09 19:16:05.307000+00:00  1.414744e+09   474.0   342.0   \n",
       "        2018-11-09 19:16:05.571000+00:00  1.414745e+09   363.0   323.0   \n",
       "        2018-11-09 19:16:05.802000+00:00  1.414745e+09   -21.0   510.0   \n",
       "        2018-11-09 19:16:06.066000+00:00  1.414745e+09    35.0   283.0   \n",
       "\n",
       "                                          IMU A3  IMU G1  IMU G2  IMU G3  \\\n",
       "ride_id UTC                                                                \n",
       "15692   2018-11-09 19:16:03.789000+00:00   110.0    75.0  -124.0   -86.0   \n",
       "        2018-11-09 19:16:04.053000+00:00    62.0    34.0   -36.0   -92.0   \n",
       "        2018-11-09 19:16:04.284000+00:00    80.0    69.0   -63.0   -42.0   \n",
       "        2018-11-09 19:16:04.548000+00:00  -104.0   192.0   -92.0   -37.0   \n",
       "        2018-11-09 19:16:04.812000+00:00   -32.0  -421.0  -233.0  -229.0   \n",
       "        2018-11-09 19:16:05.043000+00:00  -102.0  -355.0  -376.0  -397.0   \n",
       "        2018-11-09 19:16:05.307000+00:00  -219.0  -234.0  -527.0  -465.0   \n",
       "        2018-11-09 19:16:05.571000+00:00  -131.0    60.0  -662.0  -305.0   \n",
       "        2018-11-09 19:16:05.802000+00:00  -447.0    78.0  -643.0  -153.0   \n",
       "        2018-11-09 19:16:06.066000+00:00  -132.0  -114.0  -430.0   132.0   \n",
       "\n",
       "                                          IMU M1  IMU M2  IMU M3  TimeDelta  \n",
       "ride_id UTC                                                                  \n",
       "15692   2018-11-09 19:16:03.789000+00:00  -309.0   209.0    39.0        0.0  \n",
       "        2018-11-09 19:16:04.053000+00:00  -320.0   194.0    38.0      252.5  \n",
       "        2018-11-09 19:16:04.284000+00:00  -329.0   189.0    49.0      501.5  \n",
       "        2018-11-09 19:16:04.548000+00:00  -330.0   180.0    64.0      753.5  \n",
       "        2018-11-09 19:16:04.812000+00:00  -325.0   161.0    97.0     1003.5  \n",
       "        2018-11-09 19:16:05.043000+00:00  -337.0   117.0   151.0     1253.5  \n",
       "        2018-11-09 19:16:05.307000+00:00  -311.0    25.0   217.0     1504.5  \n",
       "        2018-11-09 19:16:05.571000+00:00  -238.0    -8.0   272.0     1755.5  \n",
       "        2018-11-09 19:16:05.802000+00:00  -159.0   -21.0   321.0     2006.5  \n",
       "        2018-11-09 19:16:06.066000+00:00   -86.0   -38.0   326.0     2258.5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an elapsed_timedelta field:\n",
    "\n",
    "#timedelta_values = (motion_df_dropped['Time']-motion_df_dropped['Time'][0])\n",
    "#motion_df_dropped.insert(loc=1, column='TimeDelta', value=timedelta_values, drop=True)\n",
    "motion_df_dropped['TimeDelta'] = (motion_df_dropped['Time']-motion_df_dropped['Time'][0])\n",
    "#print(elapsed_timedelta)\n",
    "#motion_df_dropped.head()\n",
    "motion_df_dropped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footage sync code written by Alina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Footage sync code written by Alina: (Miulti-Column)\n",
    "\n",
    "import time\n",
    "\n",
    "#simple method: only walking, paddling, floating, surfing\n",
    "#complex method: columns created based on footage file labels\n",
    "def label_data( footage_file = 'Footage.txt', labelling_method = 'simple', sync_threshold = 20000 ):\n",
    "    \n",
    "    # calculate sync_buf which will be used to merge the footage data and the imu data    \n",
    "    #First, perform sync\n",
    "    sync_buf = 0\n",
    "    with open(footage_file) as file:\n",
    "        # for each reading in footage file: \n",
    "        for line in file:     \n",
    "            \n",
    "            # get labeled time and format it into a time structure object            \n",
    "            labelled_time = line.split(None, 2) \n",
    "#             print('labelled_time: ', labelled_time)\n",
    "            try:\n",
    "                cur_time = time.strptime(labelled_time[0], '%M:%S')\n",
    "#                 print('curr_time: ', cur_time)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            #             \n",
    "            labelled_time[1] = labelled_time[1].rstrip()\n",
    "            if labelled_time[1].lower() == 'sync': #Assumption that first word in sync line is \"sync\"\n",
    "                sync_time = cur_time.tm_min * 60 * 1000 + cur_time.tm_sec * 1000\n",
    "                index = 0\n",
    "                start = 0\n",
    "                end = 0\n",
    "                print('sync_time: ', sync_time)\n",
    "                #Syncing occurs when IMU A2 data is negative for a longer period than the provided threshold\n",
    "                #Default is 20 seconds\n",
    "                for data in motion_df_dropped['IMU A2']:\n",
    "#                     print(data)\n",
    "                    # \n",
    "                    if data < 0 and start == 0:\n",
    "                        start = motion_df_dropped['TimeDelta'][index]\n",
    "#                         print('start: ', start)\n",
    "                    elif data > 0 and start != 0:\n",
    "                        end = motion_df_dropped['TimeDelta'][index]\n",
    "#                         print('end: ', end)\n",
    "                        \n",
    "                        # calculate the buffer between the time of sync start and the sync time                        \n",
    "                        if end - start > sync_threshold:\n",
    "                            sync_buf = start - sync_time\n",
    "                            \n",
    "                            break\n",
    "                        start = 0\n",
    "                    index += 1\n",
    "\n",
    "    accepted_labels = set()\n",
    "    \n",
    "    # if in simple label mode, create 4 classifications of footage data, if not, \n",
    "    # then just use the labels in the footage file \n",
    "    if labelling_method == 'simple':\n",
    "        accepted_labels = {'WALKING', 'PADDLING', 'FLOATING', 'SURFING'}\n",
    "        \n",
    "        # add label columns and fill with default 0        \n",
    "        #Create new DataFrame containing label info\n",
    "        label_frame = pd.DataFrame(0, index = motion_df_dropped.index, columns = accepted_labels)\n",
    "        for label in accepted_labels:\n",
    "            label_frame[label] = [0] * len(motion_df_dropped['Time'])\n",
    "    \n",
    "    \n",
    "    # use the calulated sync_buf to alignt he footage data with the imu data\n",
    "    #Convention of labelled footage text: \"MINUTE:SECOND LABEL\"\n",
    "    elapsed_time = 0\n",
    "    cur_label = ''\n",
    "    buffer = 0\n",
    "    with open(footage_file) as file:\n",
    "        for line in file:\n",
    "            \n",
    "            # if simple method, just look for first word\n",
    "            # labelled time is a tuple containing the time of event and the type of event\n",
    "            if labelling_method == 'simple':\n",
    "                labelled_time = line.split(None, 2) #simple categorizes on a one-word basis\n",
    "            else:\n",
    "                labelled_time = line.split(None, 1) #complex requires the entire label\\\n",
    "            # print('labelled_time: ', labelled_time)\n",
    "                    \n",
    "            \n",
    "            #If the first word is not a properly formatted time, the line cannot be read\n",
    "            try:\n",
    "                # format time in time structure\n",
    "                cur_time = time.strptime(labelled_time[0], '%M:%S')\n",
    "                # calculate the current time in ms using sync_buf                \n",
    "                cur_timeMS = cur_time.tm_min * 60 * 1000 + cur_time.tm_sec * 1000 + sync_buf\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            labelled_time[1] = labelled_time[1].rstrip() #Remove potential newline\n",
    "                \n",
    "            #Check for end of video and modify buffer accordingly\n",
    "            # assign the time of the 'end of video' to buffer\n",
    "            if labelled_time[1].lower() == 'end of video': #Assumption that label end video with \"end of video\"\n",
    "                buffer += cur_timeMS\n",
    "                \n",
    "            \n",
    "                \n",
    "            #----Complex \"mode\" below: --------\n",
    "                \n",
    "            #Modify accepted labels list if reading a new label and in complex mode\n",
    "            elif labelling_method == 'complex' and (labelled_time[1].upper() not in accepted_labels):\n",
    "                accepted_labels.add(labelled_time[1].upper())\n",
    "                if not cur_label:\n",
    "                    label_frame = pd.DataFrame(0, index = motion_df_dropped.index, columns = accepted_labels)\n",
    "                label_frame[labelled_time[1].upper()] = [0] * len(motion_df_dropped['Time'])\n",
    "                \n",
    "            if labelled_time[1].upper() in accepted_labels:\n",
    "                while (elapsed_time < len(motion_df_dropped['Time']) and\n",
    "                      (np.isnan(motion_df_dropped['TimeDelta'][elapsed_time]) or\n",
    "                       motion_df_dropped['TimeDelta'][elapsed_time] < cur_timeMS + buffer)):\n",
    "                    if cur_label != '':\n",
    "                        label_frame[cur_label][elapsed_time] = 1\n",
    "                    elapsed_time += 1\n",
    "                if labelled_time[1].upper() != 'end of video':\n",
    "                    cur_label = labelled_time[1].upper()\n",
    "\n",
    "    # concatinate time labels with their corresponsing measurements by time index    \n",
    "    labelled = pd.concat([motion_df_dropped, label_frame], axis = 1)\n",
    "\n",
    "\n",
    "    return labelled\n",
    "\n",
    "pd.options.display.max_rows = 5000\n",
    "pd.options.display.max_columns = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sync_time:  36000\n",
      "21645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6113"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_df_simple = label_data('../Labelled_Footage/Footage3.txt')\n",
    "print(len(motion_df_simple))\n",
    "motion_df_simple[['SURFING', 'PADDLING', 'FLOATING', 'WALKING']].sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sync_time:  36000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9169"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_df_complex = label_data('../Labelled_Footage/Footage3.txt', 'complex')\n",
    "motion_df_complex[['SYNC', 'FLOATING', 'WALKING IN WATER', 'PUSH-OFF', 'PADDLING INTO WAVES', 'SIT-UP', \"TURNING TO SURFER'S LEFT\", 'LAY-DOWN', 'PADDLING FOR A WAVE', 'POP-UP', 'SURFING', 'STEP-OFF', \"TURNING TO SURFER'S RIGHT\", 'SIT-BACK', 'OFF-BOARD', 'PADDLING', 'WIPE-OUT', 'PULL-BACK LEASH', 'PADDLING FOR POSITION', 'DISCARD', 'WALKING OUT OF WATER']].sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate multiple footage files that we have so far to create a larger mass of data samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_complex = label_data('../Labelled_Footage/Footage.txt', 'complex')\n",
    "df2_complex = label_data('../Labelled_Footage/Footage2.txt', 'complex')\n",
    "df3_complex = label_data('../Labelled_Footage/Footage3.txt', 'complex')\n",
    "df4_complex = label_data('../Labelled_Footage/Footage4.txt', 'complex')\n",
    "\n",
    "df_concatenated = pd.concat([df1_complex, df2_complex, df3_complex, df4_complex])\n",
    "\n",
    "print(\"Shape of first dataframe:\", df1_complex.shape)\n",
    "print(\"Shape of all combined dataframes:\", df_concatenated.shape)\n",
    "\n",
    "print(\"Printing dataframe...\")\n",
    "#print(df1_complex.head(10))\n",
    "df_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Raw IMU data values to real units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct IMU data\n",
    "\n",
    "#make a deep copy of motion_df_labelled\n",
    "df_converted = motion_df_complex.copy(deep = 'true')\n",
    "\n",
    "#for rows in df_corrected\n",
    "for row in range(0, df_converted.shape[0]):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #convert acceleromters (new: m/s^2)\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A1')] *= -0.019141  #forwards/backwards\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A2')] *= 0.019141   #upside down/right side up\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A3')] *= 0.019141   #sideways: negative = left, positive = right\n",
    " \n",
    "    #convert gyroscopes (new: deg/s)\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G1')] /= 8.2        #roll\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G2')] /= 8.2        #yaw\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G3')] /= 8.2        #pitch (flipping forwards/backwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop data columns that we don't care about predicting/visualizing: \n",
    "df_converted = df_converted.drop(columns=[\"FLIP BOARD RIGHT SIDE UP\", \"NEW\", \"DONE, OUT OF WATER\"])\n",
    "#df_converted = df_converted.drop(columns!=[\"SURFING, FLOATING, PADDLING INTO WAVES, PADDLING FOR A WAVE, PADDLING FOR POSITION, PADDLING\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot IMU Signals with Labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "\n",
    "#define a function that plots a column of dataf in relation to time. color coded to match labels in dataf\n",
    "#requires that:\n",
    "#dataf has a 'TimeDelta' column\n",
    "#labels: walking, surfing, floating, paddling\n",
    "\n",
    "def createPlot (dataf, column):\n",
    "    \n",
    "        #create new data frame to be plotted\n",
    "        #Only consider columns after Velocity\n",
    "        dfPlot = pd.DataFrame(columns = ['TIME'] + list(dataf)[list(dataf).index('TimeDelta') + 1:], dtype = float)\n",
    "        \n",
    "        #add timedelta column from dataf to dfPlot\n",
    "        dfPlot['TIME'] = dataf['TimeDelta']\n",
    "        \n",
    "        #get the index of the column to be graphed\n",
    "        columnInd = dataf.columns.get_loc(column)\n",
    "        \n",
    "        #for each row in dfPlot (number of IMU readings)\n",
    "        for row in range(0, dfPlot.shape[0]):\n",
    "            \n",
    "            #for the indexes of the label columns in dfPlot\n",
    "            for col in range(1, dfPlot.shape[1]):\n",
    "                \n",
    "                #if a label in the row is 1 in dataf\n",
    "                if dataf.iloc[row, dataf.columns.get_loc(dfPlot.columns[col])] == 1:\n",
    "                    \n",
    "                    #add the sensors value to the corresponding column in dfPlot\n",
    "                    dfPlot.iloc[row, dfPlot.columns.get_loc(dfPlot.columns[col])] = dataf.iloc[row, columnInd]\n",
    "                    #dfPlot.iloc[row, dfPlot.columns.get]\n",
    "        \n",
    "        #Set up colormap so that we don't see a repeat in color when graphing\n",
    "        #plt.gca().set_prop_cycle('color',plt.cm.plasma(np.linspace(0,1,dfPlot.shape[1])))\n",
    "        plt.gca().set_prop_cycle('color',plt.cm.tab20(np.linspace(0,1,dfPlot.shape[1])))\n",
    "        for col in range (1, dfPlot.shape[1]):\n",
    "            plt.plot(dfPlot['TIME'], dfPlot[list(dfPlot)[col]])\n",
    "        \n",
    "        plt.gca().legend(loc = 'lower left')\n",
    "        plt.title(column)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"IMU Data\")\n",
    "\n",
    "        #file_name = column\n",
    "        #pdf_string = '.jpg'\n",
    "        #file_name += pdf_string\n",
    "        \n",
    "        #plt.savefig(file_name)\n",
    "        plt.show()\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#For plotting, just comment out the \"concatenation\" lines. \n",
    "\n",
    "#Need to clear kernel and then only run all above so that it plots on axes directly below, rather than on another plot\n",
    "print(\"Creating Plots...\")\n",
    "createPlot(df_converted,'IMU A1')\n",
    "createPlot(df_converted,'IMU A2')\n",
    "createPlot(df_converted,'IMU A3')\n",
    "#createPlot(df_converted,'IMU G1')\n",
    "#createPlot(df_converted,'IMU G2')\n",
    "#createPlot(df_converted,'IMU G3')\n",
    "#createPlot(df_converted,'IMU M1')\n",
    "#createPlot(df_converted,'IMU M2')\n",
    "#createPlot(df_converted,'IMU M3')\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of looking at all labels, just look at floating vs. not floating: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_converted.columns))\n",
    "\n",
    "drop_list = ['SYNC', 'WALKING IN WATER', 'PUSH-OFF', 'PADDLING INTO WAVES', 'SIT-UP', \"TURNING TO SURFER'S LEFT\", 'LAY-DOWN', 'PADDLING FOR A WAVE', 'POP-UP', 'SURFING', 'STEP-OFF', \"TURNING TO SURFER'S RIGHT\", 'SIT-BACK', 'OFF-BOARD', 'PADDLING', 'WIPE-OUT', 'PULL-BACK LEASH', 'PADDLING FOR POSITION', 'DISCARD', 'WALKING OUT OF WATER']\n",
    "for x in drop_list: \n",
    "    if x in df_converted.columns:\n",
    "        df_converted = df_converted.drop(columns=[x])\n",
    "                                          \n",
    "# print(list(df_converted.columns))\n",
    "\n",
    "                                          \n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Creating Plots...\")\n",
    "createPlot(df_converted,'IMU A1')\n",
    "createPlot(df_converted,'IMU A2')\n",
    "createPlot(df_converted,'IMU A3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_converted[\"IMU A2\"]\n",
    "df_converted[\"TimeDelta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can try analyzing a \"floating\" time-slice in the middle of this data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at IMU A2 time-slice from 1,050,000 to 1,300,000:\n",
    "\n",
    "keys = df_converted[\"TimeDelta\"]\n",
    "values = df_converted[\"IMU A2\"]\n",
    "imu_a2_time_dict = dict(zip(keys, values))\n",
    "\n",
    "#Now need to slice the dictionary into 2 new lists: \n",
    "time_slice = []\n",
    "imu_a2_slice = []\n",
    "for time in imu_a2_time_dict:\n",
    "    if time > 1050000 and time < 1300000:\n",
    "        time_slice = float(time)\n",
    "        #print(imu_a2_time_dict[time] - 9.80665)\n",
    "        #Need to also subtract gravity from IMU A2 value:\n",
    "        imu_a2_slice = float(imu_a2_time_dict[time] - 9.80665)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Plot not showing up for some reason - axis tick issue?\")\n",
    "\n",
    "plt.plot(x1=time_slice, y1=imu_a2_slice, color=\"black\", linewidth=2, markersize=12)\n",
    "#plt.xticks=(np.arange(1050000, 1300000, step=50000))\n",
    "plt.xlabel('Time Elapsed [ms]')\n",
    "plt.ylabel('IMU A2')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detrend the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dacc_array = signal.detrend(imu_a2_slice)\n",
    "f_s = 5.0 #sampling frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data: \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy from: http://localhost:8890/notebooks/BuoyCallibratorData_DoubleIntegralAnalysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#According to historic CDIP data: \n",
    "#http://cdip.ucsd.edu/offline/wavecdf/wnc_browse.php?ARCHIVE/201p1/201p1_historic+waveHs+201811\n",
    "#Expecting significant wave height between 0.5 and 0.7m \n",
    "#Expecting wave period between 7s and 12s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
